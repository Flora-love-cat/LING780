{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Embedding Representations from Transformers)\n",
    "\n",
    "**contextualized embedding**: BERT produce word embeddings or sentence embeddings which capture bidirectional context info in a sentence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretraining**: unsupervised pretraining on a large corpus to learn general language understanding.\n",
    "\n",
    "- Masked Language Modelling (MLM): Predicting masked tokens (15%) in a sentence. Loss is only evaluated for [CLS] and masked-out words\n",
    "\n",
    "    learn relationships between multiple words in a sentence.\n",
    "    \n",
    "    - 80% of time: masked out words are replaced by [MASK]\n",
    "    \n",
    "    - 10% of time: randomly replaces masked out words.\n",
    "\n",
    "    - 10% of time: recover the masked out words.\n",
    "    \n",
    "- Next Sentence Prediction (NSP): Predicting if a given sentence B follows sentence A in the original text. a type of sentence-pair classification.\n",
    "\n",
    "    learn relationships between multiple sentences.\n",
    "\n",
    "    input: 2-sentence pairs. output: binary (Yes or No)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fine-tuning**: on various NLP tasks with small labeled data by **adding a task-specific output layer on top of BERT encoder.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture: Encoder-only Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- multiple stacks of encoder block (base 12, large 24)\n",
    "\n",
    "- each Encoder block has 2 sub-layers: a multi-head self-attention, a position-wise FC feedforward network.\n",
    "\n",
    "\n",
    "- **Input Representation**: WordPiece tokenization and combines it with positional encoding and segment encoding.\n",
    "\n",
    "  - Token Embeddings: $E \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  - Positional Embeddings: $P \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  - Segment Embeddings: $S \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  $m$: input sequence length (context size)\n",
    "\n",
    "  first token is `[CLS]` (classification)\n",
    "  \n",
    "- **Output**: token-level or sequence-level representations depending on the task\n",
    "\n",
    "  Output of each encoder layer along each token's path is an embedding for that token, Which output to use depends on the task\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment embeddings are used for next sentence prediction pre-training task\n",
    "\n",
    "For each token in the input sequence, a segment embedding is added depending on the sequence it belongs to.\n",
    "\n",
    "help model distinguish between the two input sequences "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple example of using the pre-trained BERT model from the Hugging Face's `transformers` library in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load BERT tokenizer and pre-trained base model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "text = \"Here is an example of using BERT in PyTorch.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Run the text through the pre-trained BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# sentence representation : last hidden state of [CLS] token\n",
    "# It captures the aggregated contextual information from the entire input sequence.\n",
    "# It provides a fixed-size vector (batch_size, hidden_size) that can be used as input to a classifier for downstream tasks.\n",
    "sentence_representation = outputs.last_hidden_state[:, 0, :]  # (batch_size, sequence_length, hidden_size)\n",
    "# [CLS] token is always the first token in the input sequence\n",
    "\n",
    "print(sentence_representation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contextual embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contextual embeddings/BERT embeddings of input sequence $w_1, ..., w_n$ are the outputs of the last encoder block $h_1, ..., h_n$. \n",
    "\n",
    "$h_1$ is contextual embedding of [CLS] token, represent embedding for the whole input sequence\n",
    "\n",
    "- next sentence prediction: $h_1$ is input for FeedFward NN\n",
    "\n",
    "- fine-tuning on classification: $h_1$ is input for FeedFward NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• RoBERTa: Facebook’s (improved) version of BERT\n",
    "\n",
    "• DistilBERT, ALBERT: Smaller versions of BERT\n",
    "\n",
    "• CamemBERT, FlauBERT: BERT in French\n",
    "\n",
    "• PhoBERT, herBERT: BERT in Vietnamese and Polish, resp.\n",
    "\n",
    "• mBERT: BERT in 104 languages\n",
    "\n",
    "• SpanBERT: BERT for phrase-level tasks (e.g., named entity recognition, coreference resolution, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
