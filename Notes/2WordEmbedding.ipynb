{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding algorithm: a function $f: V \\rightarrow R^n$ mapping from vocabulary to n-D vector space\n",
    "\n",
    "- Any algorithm that performs dimensionality reduction can be used to construct a word embedding\n",
    "\n",
    "- each word embedding $f(w)$ is the result of mapping one word in vocabulary to $R^n$\n",
    "\n",
    "    i.e. hidden feature vectors extracted from data that represent word meanings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importance of word embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- represent word as 1-hot encoding vector don't contain any semantic info and has too large dimension.\n",
    "\n",
    "- manually define **features for word** is hard\n",
    "\n",
    "- word embedding represent words as **semantic and syntactic meaningful** vectors in low-dimensional feature space\n",
    "\n",
    "- Word embeddings are widely used in various NLP tasks as input features, to help models understand and process natural language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding algorithms can be divided into 2 types:\n",
    "\n",
    "- count-based: matrix factorization\n",
    "\n",
    "    - SVD: $X = USV^T$, $X$ is word-contex matrix, embedding is $US$. \n",
    "    \n",
    "        variants: PMI, PPMI, SPPMI. not use NN\n",
    "\n",
    "    - GloVe: $X = WC^T$, $X$ is co-occurrence matrix, embedding is $W$. use NN\n",
    "\n",
    "- prediction-based: embedding is output of hidden layer\n",
    "\n",
    "    - word2vec: skip-gram (SGNS), CBOW. 3-layer MLP\n",
    "\n",
    "    - FastText: word2vec extention. unit: character n-grams. can learn embedddings of out-of-vocabulary word.\n",
    "\n",
    "    - ELMo: bidirectional LSTM\n",
    "\n",
    "    - BERT: encoder-only Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspect                     | Count-based                          | Prediction-based                      |\n",
    "|----------------------------|--------------------------------------|---------------------------------------|\n",
    "| Methodology                | Co-occurrence statistics             | Predictive models                     |\n",
    "| Example algorithms         | Co-occurrence matrix, GloVe          | Word2Vec, FastText, ELMo, BERT        |\n",
    "| Computational complexity   | Offline processing, matrix factorization | Online processing, iterative training |\n",
    "| Scalability                | Can be less scalable                  | More scalable                         |\n",
    "| Handling rare words        | Dependent on co-occurrence frequency | Subword information can help (FastText)|\n",
    "| Context sensitivity        | Limited, fixed-size context window    | Better, especially with ELMo, BERT    |\n",
    "| Training data efficiency   | Can be more efficient                 | May require larger datasets           |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all the word embedding algorithms rely on Distributional Semantics Hypothesis to learn meaningful representations of words.\n",
    "\n",
    "- Distributional semantics: measure semantic similarities between linguistic items based on their distributional properties in large samples of language data. \n",
    "\n",
    "- **distributional hypothesis** is basic idea of Distributional semantics\n",
    "\n",
    "    first proposed by (Harris, 1954) \"a word is characterized by the company it keeps\"\n",
    "\n",
    "    interpretation: \n",
    "    \n",
    "    - linguistic items with similar distributions have similar meanings.\n",
    "\n",
    "    - **words that occur in same contexts have similar meanings**\n",
    "\n",
    "        here, contexts are also words that surround target words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effectiveness of word embedding algorithm in capturing semantic and syntactic relationships in language can be evaluated in a combination of intrinsic and extrinsic methods.\n",
    "\n",
    "\n",
    "Intrinsic Evaluation:\n",
    "\n",
    "- Word Similarity: Comparing 2-word cosine similarity between word embeddings with human-annotated similarity scores in benchmark datasets like WordSim-353, SimLex-999, or MEN.\n",
    "\n",
    "    $$\n",
    "    \\text{cosine similarity} = \\frac{\\phi(A)\\phi(B)}{||\\phi(A)|| ||\\phi(B)||}\n",
    "    $$\n",
    "\n",
    "- Word Analogy: Testing embeddings' ability to solve analogy tasks, such as \"A is to B as C is to D\" (e.g., \"man\" is to \"woman\" as \"king\" is to \"queen\"). \n",
    "\n",
    "    A popular benchmark for this task is the Google Word Analogy dataset.\n",
    "\n",
    "\n",
    "Extrinsic Evaluation: performance of downstream NLP tasks taking word embeddings as input features.\n",
    "\n",
    "- Text classification (e.g., sentiment analysis, topic categorization)\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "- Part-of-Speech (POS) Tagging\n",
    "\n",
    "- Semantic Role Labeling (SRL)\n",
    "\n",
    "- Machine Translation\n",
    "\n",
    "- Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analogies 类比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- syntactic analogies: A is to A' as B is to B'\n",
    "\n",
    "- task: predict B' from vocabulary\n",
    "\n",
    "e.g.\n",
    "\n",
    "- `king` is to `man` as `?` is to `woman`\n",
    "\n",
    "- `Bill Gates` is to `Microsoft` as `?` is to `Google`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- equation:\n",
    "\n",
    "    $$\n",
    "    \\phi(king)-\\phi(man)\\approx \\phi(?)-\\phi(woman)\n",
    "    $$\n",
    "\n",
    "- objective: minimize the $l_2$ norm\n",
    "\n",
    "    $$\n",
    "    \\hat w = \\arg \\min_w \\left \\| \\phi(king)-\\phi(man)+ \\phi(woman)-\\phi(w) \\right \\|^2\n",
    "    \\\\[1em]\n",
    "    \\Rightarrow \\hat w = queen\n",
    "    $$\n",
    "\n",
    "- alternative objective: maximize 2 similarities and one difference\n",
    "\n",
    "    $$\n",
    "    \\arg \\max_{b'} \\cos(b', b-a+a') = \\cos(b', b)-\\cos(b', a)+\\cos(b', a')\n",
    "    $$\n",
    "\n",
    "- metric: accuracy\n",
    "\n",
    "    true B' is optimal result of Levy and Goldberg's similarity multiplication method\n",
    "    \n",
    "    $$\n",
    "    B' = \\arg \\max_{B'} \\frac{\\cos(B', A')\\cos(B',B)}{\\cos(B',A)+\\epsilon}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gender bias can occur in the word\n",
    "\n",
    "- extreme **female** occupations\n",
    "\n",
    "    homemaker, nurse, receptionist, librarian, socialite, hardresser, nanny, bookkeeper, stylist, housekeeper, interior designer, guidance counselor\n",
    "\n",
    "- extreme **male** occupations\n",
    "\n",
    "    maestro, skipper, protege, philosopher, captain, architest, financier, warrior, broadcaster, magician, figher pilot, boss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper: [*Neural word embedding as implicit matrix factorization*](https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare 5 word embedding algorithms (SGNS, NCE, SPMI, SPPMI, rank-d SVD) \n",
    "\n",
    "these algorithm can all be viewed as implicit matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) sparse or dense\n",
    "\n",
    "    sparse: PPMI, SPPMI, SVD\n",
    "\n",
    "    dense: SGNS\n",
    "\n",
    "2) weighted or unweighted\n",
    "\n",
    "    weighted: SGNS, SPMI, SPPMI. gives more weight to frequent pairs\n",
    "\n",
    "    unweighted: SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe\n",
    "\n",
    "$$\n",
    "X = WC^T\n",
    "$$\n",
    "\n",
    "- SGNS/SPMI\n",
    "\n",
    "$$\n",
    "M = \\text{PMI}(w,c)-\\log k\n",
    "$$\n",
    "\n",
    "\n",
    "- SPPMI: \n",
    "\n",
    "$$\\text{SPPMI} = \\text{PPMI}-\\log k$$\n",
    "\n",
    "\n",
    "- SVD: \n",
    "\n",
    "$$\\text{SPPMI}_d = U_d \\Sigma_d V_d^T$$\n",
    "\n",
    "- NCE\n",
    "\n",
    "$$\n",
    "M = \\log P(w|c)-\\log k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ad of SVD (dense low-dimensional vectors)\n",
    "    \n",
    "1) SVD is exact, don't need learning rates or tuning hyperparameter $k$\n",
    "\n",
    "2) SVD is easy to train on count-aggregated data, i.e., $\\left\\{(w,c,\\#(w,c)) \\right\\}$ triplets\n",
    "\n",
    "    thus scalable to larger corpora\n",
    "\n",
    "    while SGNS needs each observed $(w,c)$ pair to be presented separately\n",
    "    \n",
    "3) improved computational efficiency\n",
    "    \n",
    "4) better generalization\n",
    "       \n",
    "     \n",
    "dis of SVD\n",
    "\n",
    "1) SVD suffer from unobserved values, which are very common in word-context matrices\n",
    "\n",
    "    SGNS can distinguish between observed and unobserved word-context pairs\n",
    "\n",
    "2) exact weighted SVD is a hard computational problem\n",
    "\n",
    "    while SGNS's training objective weighs word-context pairs with different frequency differently,\n",
    "\n",
    "    assign more optimal values to frequent pairs while allow more error for infrequent pairs\n",
    "\n",
    "3) SVD needs matrix $M$ to be sparse\n",
    "\n",
    "    while SGNS don't require that coz it only cares about **observed and sampled word-context pairs**\n",
    "\n",
    "    SGNS can optimize **dense matrix**, e.g. $M=PMI-\\log k$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
