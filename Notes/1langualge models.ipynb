{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost of NLP model development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create labelled dataset: expensive human labor\n",
    "\n",
    "- train model: computation (PFLOPs)\n",
    "\n",
    "- inference: user computation time (ms/example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perplexity 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**perplexity**\n",
    "\n",
    "- Perplexity measures how well a language model predicts a sequence of words by quantifying the **average number of choices (the number of equally likely choices ) the model has for each word in the sequence**.\n",
    "\n",
    "- the lower perplexity, the better the model: A lower perplexity means that the model is more certain about the next word in the sequence, because it assigns a higher probability to the correct word and lower probabilities to other words.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Perplexity of a language model parametrized by $\\theta$ is defined as n-power root of likelihood (joint probability) of a sequence of words $w_1, w_2, ..., w_N$ under such model\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(\\theta)=\\sqrt[n]{\\prod _{i=1}^N p_{\\theta} (w_i | w_{1:i-1})}\n",
    "$$\n",
    "\n",
    "where $p_{\\theta}(w_i | w_{1:i-1})$ represents conditional probability of observing word $w_i$ given the previous words $w_{1:i-1}$ (context), according to the language model parametrized by $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**interpretation**\n",
    "\n",
    "- log perplexity is Negative log likelihood\n",
    "\n",
    "    $$\n",
    "    \\log(\\text{Perplexity}(\\theta))=\\log \\left ( \\prod _{i=1}^N p_{\\theta} (w_i | w_{1:i-1}) \\right )^{-\\frac{1}{N}}=-\\frac{1}{N}\\log \\left ( \\prod _{i=1}^N p_{\\theta} (w_i | w_{1:i-1}) \\right )\n",
    "    $$\n",
    "\n",
    "- perplexity is exponential of average cross-entropy loss\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\text{Perplexity}(\\theta) &= \\exp \\left[\\log(\\text{Perplexity}(\\theta))\\right]\\\\[1em]\n",
    "    &= \\exp\\left[-\\frac{1}{N}\\log \\left ( \\prod_{i=1}^N p_{\\theta} (w_i | w_{1:i-1}) \\right )\\right]\\\\[1em]\n",
    "    &= \\exp\\left[\\frac{1}{N}\\sum_{i=1}^N - \\log p_{\\theta} (w_i | w_{1:i-1})\\right]\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e.g. if perplexity is 100, then model is as uncertain about the next word in a sequence as it would be if there were 100 equally likely words to pick from.\n",
    "\n",
    "$$\n",
    "\\text{perplexity}(\\theta)=\\left ( \\prod _{i=1}^{100} p_{\\theta} (w_i | w_{1:i-1}) \\right )^{-\\frac{1}{100}}=\\left ( \\left(\\frac{1}{100}\\right)^{100}  \\right )^{-\\frac{1}{100}}=100\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU (BiLingual Evaluation Understudy) score\n",
    "\n",
    "$$\n",
    "\\text{BLEU}=\\min(1, \\exp(1-\\frac{len(ref)}{len(pred)})) \\left(\\prod_{n=1}^4 \\text{n-gram precision}\\right)^{1/4}\n",
    "$$\n",
    "\n",
    "$\\min(1, \\exp(1-\\frac{len(ref)}{len(pred)}))$ is length penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "character, word, sentence, paragraph, document\n",
    "\n",
    "Subword: A unit between character and word, representing meaningful parts or morphemes. Used to handle out-of-vocabulary words, morphologically rich languages, and improve generalization. Algorithms like BPE, WordPiece, and SentencePiece split words based on frequency.\n",
    "\n",
    "Token: A single unit in text, typically a word, subword, or character. Tokenization breaks text into smaller units called tokens.\n",
    "\n",
    "Gram: can be n-gram characters or n-gram tokens, E.g., bigram tokens in sentence \"I like cats.\" are \"I like\", \"like cats\" and \"cats.\"\n",
    "\n",
    "Segment: A continuous part of text divided by semantic or syntactic boundaries. Can be interchangeable with \"sentence\" or \"phrase\".\n",
    "\n",
    "Sentence: A grammatically complete sequence of words, typically separated by punctuation marks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A language model is designed to assign  a probability distribution over a sequence of words, ahieved by modeling the conditional probability of a word given its preceding words (history) in the sequence.\n",
    "\n",
    "  $$\n",
    "  p(w_1,...,w_n)=p(w_1)p(w_2|w_1)...p(w_n|w_1,...,w_{n-1})\n",
    "  $$\n",
    "\n",
    "\n",
    "- As the length of the sequence (n) increases, the number of histories (possible unique sequences of words) grow exponentially as \n",
    "\n",
    "  $$|V|^{n-1}$$\n",
    "\n",
    "  where $|V|$ is vocabulary size\n",
    "\n",
    "\n",
    "-  total number of free parameters in the model for the last conditional probability is \n",
    "\n",
    "  $$(|V|-1)|V|^{n-1}$$\n",
    "\n",
    "\n",
    "- To reduce the num of params, we need to group histories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram, Bigram, Trigram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let $\\pi_n: V^n \\rightarrow C$ be a mapping from word seq of length $n$ to a finite set $C$ (a context)\n",
    "\n",
    "\n",
    "- then language model becomes\n",
    "\n",
    "    $$\n",
    "    p(w_{n+1}|w_1,...,w_n)=p(w_{n+1}|\\pi_n(w_1,...,w_n))\n",
    "    $$\n",
    "\n",
    "\n",
    "- num of params is\n",
    "\n",
    "    $$\n",
    "    O(|V|\\cdot |C|)\n",
    "    $$\n",
    "\n",
    "    where $|C|$ is context window size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unigrams: words are independent with each other, we don't listen to any history\n",
    "\n",
    "$$\n",
    "\\pi(w_1,...,w_n)=\\varnothing \n",
    "$$\n",
    "\n",
    "    \n",
    "\n",
    "- bigrams: only listen to the last word $w_n$\n",
    "\n",
    "$$\n",
    "\\pi(w_1,...,w_n)=w_n\n",
    "$$\n",
    "\n",
    "   \n",
    "- trigrams: only listen to the last 2 words $w_{n-1}, w_n$\n",
    "\n",
    "$$\n",
    "\\pi(w_1,...,w_n)=(w_{n-1}, w_n)\n",
    "$$\n",
    "\n",
    "    \n",
    "- num of prams grows as \n",
    "\n",
    "$$O(|V|), O(|V|^2), O(|V|^3)$$\n",
    "\n",
    "\n",
    "- num of prams in topics for LDA\n",
    "\n",
    "    $$\n",
    "    O(K \\cdot |V|)\n",
    "    $$\n",
    "\n",
    "    where $K$ is num of topics in a corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLE: problematic\n",
    "\n",
    "\n",
    "- Bayesian - Dirichlet prior: often used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the maximum likelihood estimate of a trigram model is\n",
    "\n",
    "$$\n",
    "\\hat p(w_3 | w_1, w_2) =\\frac{p(w_1,w_2,w_3)}{p(w_1,w_2)}= \\frac{\\frac{\\text{count}(w_1,w_2,w_3)}{ \\#trigram}}{\\frac{\\text{count}(w_1,w_2)}{\\# bigram}} \\approx\\frac{\\text{count}(w_1,w_2,w_3)}{\\text{count}(w_1,w_2)}\n",
    "$$\n",
    "\n",
    "\n",
    "- problem: in training set, word 1, 2, 3 may not occur together, which leads to $\\text{count}(w_1,w_2,w_3)=0$\n",
    "    \n",
    "\n",
    "- probabilities need to be **\"smoothed\"** to avoid zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian: Dirichlet prior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta \\in (-3, 10)$ is added weight\n",
    "\n",
    "$$\n",
    "\\hat p(w_3 | w_1, w_2) \\propto \\text{count}(w_1,w_2,w_3) + \\eta\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mixture model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a mixture model:  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(w_3 | w_1, w_2) \n",
    "&=\\text{trigram model + bigram model + unigram model} \\\\[1em]\n",
    "&= \\lambda _3 \\hat p(w_3 | w_1, w_2)  + \\lambda _2 \\hat p(w_3 | w_2) +\\lambda _1 \\hat p(w_3 ) \\\\[1em]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\lambda_i$ is weight for model $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class-based bigram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouping words into **classes** and estimating the **class-based model** gives meaningful clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model:\n",
    "\n",
    "$$\n",
    "p(w_2 | w_1) = p(\\text{class}(w_2)|\\text{class}(w_1)) p(w_2|\\text{class}(w_2))=p(c_2|c_1)p(w_2|c_2)\n",
    "$$\n",
    "\n",
    "where $c_i$ is class (topic) for word $w_i$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use bottom-up agglomerative clustering to group words\n",
    "\n",
    "\n",
    "- bigram model gives highest likelihood (no grouping)\n",
    "\n",
    "\n",
    "- each step: merge the pair of classes that decrease likelihood (perplexity) of the data least\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics is study of meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's *meaning*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leonard Bloomfield: Yale professor of Structuralism Linguistics\n",
    "\n",
    "    view: words have a **precise and technical** meaning that can only be uncovered by **experts** through careful study, ordinary people can't know true meaning of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ontologies implement Bloomfield's idea of meaning\n",
    "\n",
    "    a huge database that contains everything a typical person might know in a **machine-readable** format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e.g.\n",
    "\n",
    "    - WordNet: a database that contains relationships between different word meanings\n",
    "\n",
    "    - Amazon Alexa: ontology objects convert natural-language requests into an abstract representation of what the user wants Alex to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometry of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define following 4 kinds of pizza vectors. \n",
    "# feature: 13 kinds of toppings\n",
    "import numpy as np\n",
    "# white pie with meatballs\n",
    "p = np.array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
    "\n",
    "# plain white pie\n",
    "q = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "\n",
    "# red pie with meatballs\n",
    "r = np.array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "# pain red pie\n",
    "s = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition\n",
    "\n",
    "$$\\mathbf{p} + \\mathbf{q}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p + q # addition of toppings of 2 pizzas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard product \n",
    "\n",
    "$$\\mathbf{p} \\odot \\mathbf{q}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p * q # intersection of toppings of 2 pizzas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{max}(\\mathbf{p}, \\mathbf{q})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.maximum(p, q) # union of toppings of 2 pizzas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product\n",
    "\n",
    "$$\\mathbf{p} \\cdot \\mathbf{q}=\\mathbf{p}^T \\mathbf{q}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p@q # number of overlap toppings of 2 pizzas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtraction\n",
    "\n",
    "$$\\mathbf{p} - \\mathbf{q}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p-q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- meaning of subtraction\n",
    "\n",
    "    some elements of resulting vector after subtraction may be -1\n",
    "\n",
    "    you may think it doesn't make sense for a pizza having -1 units of topping,\n",
    "    \n",
    "    but subtraction can describe **analogies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in NLP, often rewrite equation to be \n",
    "\n",
    "    $$\\mathbf{p} - \\mathbf{q} + \\mathbf{s} = \\mathbf{r}$$\n",
    "\n",
    "    vector $- \\mathbf{q} + \\mathbf{s} $ represents the operation of replacing the properties of a plain white pie ($\\mathbf{q}$) with those of a plain red pie ($\\mathbf{s}$)\n",
    "\n",
    "    white pie with meatballs VS. plain white pie  = red pie with meatballs VS. plain red pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.isclose(p-q+s, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use PCA to embed pizza vectors from $\\mathbb{R}^{13}$ into $\\mathbb{R}^2$\n",
    "\n",
    "\n",
    "- this embedding is approximately **isometric** w.r.t. absolute value of cosine distance and Euclidean distance\n",
    "    \n",
    "    \n",
    "- the closer 2 pizzas appear to one another in the plot,\n",
    "\n",
    "    the greater their cosine similarity\n",
    "        \n",
    "        \n",
    "- Note: the 2 axes have no particular interpretation, \n",
    "    \n",
    "    coz they're both hidden/latent variables, \n",
    "        \n",
    "    so the actual coordinate values don't mean anything,\n",
    "    \n",
    "    the only important thing is **distance between points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Sklean in silent mode and automatically say yes\n",
    "# !yes|pip install -U scikit-learn -q\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create pizza matrix P\n",
    "row_idx = ['White Clam Pizza', 'Original Tomato Pie', 'Margherita', 'Fresh Tomato Pie', 'Meatball & Ricotta']\n",
    "p = np.array([\n",
    "    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1]])\n",
    "\n",
    "df = pd.DataFrame(data=p, index=row_idx)\n",
    "\n",
    "# create a PCA with Number of components to keep 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# get 2D embedding of pizza matrix\n",
    "embedded = pca.fit_transform(df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# create a scatter plot for PCA-embedded pizza vectors\n",
    "ax.scatter(embedded[:,0], embedded[:,1])\n",
    "\n",
    "# Move left y-axis and bottom x-axis to centre, passing through (0,0)\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "# Eliminate upper and right axes\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "\n",
    "\n",
    "\n",
    "# annotate all the points in the scatterplot using their row index\n",
    "for i, label in enumerate(row_idx):\n",
    "    ax.annotate(label, (embedded[i, 0], embedded[i, 1]), (embedded[i, 0]-0.2, embedded[i, 1]-0.2), fontsize=10)\n",
    "\n",
    "plt.title(\"Frank Pepe Pizzas by PCA\")\n",
    "plt.xlabel(\"PCA1\", x=1)\n",
    "plt.ylabel(\"PCA2\", y=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- call `components_` attribute to see the 2 principal components used by PCA object `pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28534923, -0.52316423, -0.22554482,  0.23781501, -0.22554482,\n",
       "         0.28534923,  0.52316423, -0.        , -0.17801059,  0.10733863,\n",
       "        -0.        ,  0.23781501, -0.17801059],\n",
       "       [ 0.25736834,  0.04375923, -0.2192067 , -0.30112757, -0.2192067 ,\n",
       "         0.25736834, -0.04375923,  0.        ,  0.33928921,  0.59665754,\n",
       "         0.        , -0.30112757,  0.33928921]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verify these 2 directions are both unit vextors\n",
    "np.linalg.norm(pca.components_, axis=1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 19 2021, 11:01:55) \n[Clang 10.0.0 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
