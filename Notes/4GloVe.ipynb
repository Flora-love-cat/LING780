{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe (Global Vectors for word representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper: https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "- GloVe is an unsupervised learning algorithm\n",
    "\n",
    "- similar to PCA (find a low-rank matrix approximation)\n",
    "\n",
    "- ad: **stochastic gradient descent** can be used more efficiently\n",
    "\n",
    "- perform well in word analogies task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: **ratios of word-word co-occurrence probabilities** is semantic meaningful\n",
    "\n",
    "e.g., consider the co-occurrence probabilities for **target words** \"ice\" and \"steam\" with various **probe words** (solid, gas, water, fashion) from the vocabulary. \n",
    "\n",
    "Note:\n",
    "\n",
    "- ice is related to solid\n",
    "\n",
    "- steam is related to gas\n",
    "\n",
    "- both ice and steam are related to water\n",
    "\n",
    "- both ice and steam are unrelated to fashion\n",
    "\n",
    "\n",
    "Here are some actual probabilities from a 6 billion word corpus:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/table.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one might expect, **ice** co-occurs more frequently with **solid** than it does with **gas**, \n",
    "\n",
    "whereas **steam** co-occurs more frequently with **gas** than it does with **solid**. \n",
    "\n",
    "Both words co-occur with their shared property **water** frequently, \n",
    "\n",
    "and both co-occur with the **unrelated** word **fashion** infrequently. \n",
    "\n",
    "Only in the **ratio of probabilities**, does noise from **non-discriminative** words like **water and fashion** cancel out, \n",
    "\n",
    "so that **large values (>> 1)** correlate well with properties specific to **ice**, \n",
    "\n",
    "and **small values (<< 1)** correlate well with properties specific of **steam**. \n",
    "\n",
    "In this way, the ratio of probabilities encodes some crude form of meaning associated with the abstract concept of thermodynamic phase. 热力学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective: optimize over embedding function $\\phi$ to minimize weighted least square error (reconstruction error)\n",
    "\n",
    "    i.e., use dot product of word embeddings as a low-rank matrix approximation of logarithm of words co-occurrence $\\log(C_{ij})$\n",
    "\n",
    "- a log-bilinear model\n",
    "\n",
    "$$\n",
    "\\hat \\phi = \\arg \\min_{\\phi}L(\\phi)=\\arg \\min_{\\phi} \\sum_{i=1}^{|V|}\\sum_{j=1}^{|V|}f(C_{i, j})\\left [ \\phi(w_i)^T \\phi(w_j)-\\log C_{i, j}\\right ]^2\n",
    "$$\n",
    "\n",
    "where $\\phi(w_i) \\in \\mathbb{R}^d$ is embedding of word $i$\n",
    "\n",
    "\n",
    "$\\phi(w_i)^T \\phi(w_j)=\\left \\langle \\phi(w_i), \\phi(w_j) \\right \\rangle$ is inner product of embeddings of word $i$ and word $j$\n",
    "\n",
    "$|V|$ is size of vocabulary\n",
    "\n",
    "- $C \\in \\mathbb{R}^{|V| \\times |V|}$ is a **global word-word co-occurrence matrix**, a square matrix\n",
    "\n",
    "    where $C_{i, j}$ is the ij th entry of the matrix: co-occurrence count, the number of times target word $w_i$ occurs with the context of word $w_j$ together\n",
    "\n",
    "    which tabulates how frequently words co-occur with one another in a given corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f(x)$ is weight function\n",
    "\n",
    "    for infrequent word-context pair, weight increase to prevent only learning from extremely frequent word pairs.\n",
    "\n",
    "$$\n",
    "f(x)=\\min \\left(1, \\left(\\frac{x}{x_{max} }\\right)^{\\alpha} \\right) <= 1\n",
    "$$\n",
    "\n",
    "where $x_{max}$ is the max entry of co-occurence matrix $C$\n",
    "\n",
    "empirically set $\\alpha = \\frac{3}{4} >0$\n",
    "\n",
    "if $0<\\frac{x}{x_{max}} \\ll 1$, e.g. $\\frac{x}{x_{max}} = 10^{-4}$, \n",
    "\n",
    "it will boost $f(x) = \\min\\left(1, \\left(10^{-4}\\right)^{\\frac{3}{4}} \\right)=10^{-3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- positive: high correlation/similarity, \"Micky\" and \"Mouse\"\n",
    "    \n",
    "\n",
    "- negative: low correlation/similarity, \"Micky\" and \"Egg\"\n",
    "\n",
    "\n",
    "human can easily come up \"Mouse\" with \"Micky\", but hard to come up \"Egg\" with \"Micky\", thus positive correlation is more meaningful than negative correlation, \n",
    "    \n",
    "**thus we can set negative values in the word-context matrix to be 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relate with Analogies task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this objective associates (logarithm of) ratios of co-occurrence probabilities $\\frac{P_{ik}}{P_{jk}}$ with vector differences in the embedding space $\\phi(w_i) - \\phi(w_j)$. \n",
    "\n",
    "which means vector difference encodes same meaning of ratios\n",
    "\n",
    "$$\n",
    "[\\phi(w_i) - \\phi(w_j)]^T \\phi(w_k) =  \\log \\frac{P_{ik}}{P_{jk}} = \\log P_{ik} - \\log P_{jk} = \\log \\left(\\frac{C_{ik}}{\\sum_{k=1}^{|V|}C_{ik}}\\right)- \\log \\left(\\frac{C_{jk}}{\\sum_{k=1}^{|V|}C_{jk}}\\right)\n",
    "$$\n",
    "\n",
    "where $w_k \\in \\mathbb{R}^{d}$ is context word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{ik}=\\frac{C_{ik}}{\\sum_{k=1}^{|V|}C_{ik}}$ is co-occurrence probability by row normalizing co-occurrnce count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- full gradient descent\n",
    "\n",
    "    https://www.youtube.com/watch?v=mC7zSvYj60g\n",
    "\n",
    "\n",
    "- use pytorch to compute gradient + AdapGrad\n",
    "\n",
    "    https://towardsdatascience.com/a-comprehensive-python-implementation-of-glove-c94257c2813d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating co-occurrence matrix requires a single pass through the entire corpus\n",
    "\n",
    "For large corpus, this pass can be computationally expensive \n",
    "\n",
    "Subsequent training iterations are faster bc the co-occurrence matrix is sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 squares of sparse matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- co-occurrence matrix $C \\in \\mathbb{R}^{3292 \\times 3292}$ is squares of sparse matrix $X \\in \\mathbb{R}^{18111 \\times 3292}$\n",
    "$$\n",
    "C = X^T X\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadstring = '/Users/wenxinxu/Desktop/SDS565/data/playlists/'\n",
    "with open(leadstring+'artists.txt','r') as f:\n",
    "    artists=f.readlines()\n",
    "pl = pd.read_csv(leadstring+'playlists.txt',header=None)\n",
    "\n",
    "# a dict mapping code to artist e.g., 941: 'By The Tree'\n",
    "codetoartist = {j : artists[j].strip() for j in range(len(artists))}\n",
    "\n",
    "# a dict mapping artist to code e.g., 'By The Tree': 941\n",
    "artisttocode = {artists[j].strip() : j for j in range(len(artists))}\n",
    "\n",
    "# create sparse matrix X (18111, 3292) \n",
    "d = pl.to_dict()[0]\n",
    "inds = [(j,[int(k) for k in d[j].strip().split(' ')]) for j in range(len(d))]\n",
    "vals = np.ones(len([k for j in inds for k in j[1]])) # (189900,)\n",
    "i2 = [([j[0]]*len(j[1]),j[1]) for j in inds]\n",
    "\n",
    "row_ind = [k for j in i2 for k in j[0]]\n",
    "col_ind = [k for j in i2 for k in j[1]]\n",
    "\n",
    "X = scipy.sparse.csr_matrix((vals,(row_ind, col_ind))) # (18111, 3292)  Compressed Sparse Row matrix  X[row_ind[k], col_ind[k]] = data[k]\n",
    "# co-occurence matrix  (3292, 3292)\n",
    "C = X.T @ X\n",
    "C.setdiag(0) # for the same artist, set the co-occurrence to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadstring = '/Users/wenxinxu/Desktop/SDS565/data/playlists/'\n",
    "with open(leadstring+'artists.txt','r') as f:\n",
    "    artists=f.readlines()\n",
    "\n",
    "with open(leadstring+'playlists.txt','r') as f:\n",
    "    sentences=f.readlines()\n",
    "\n",
    "# codes of playlist without row index\n",
    "sentenceslist = [list(map(int, j.strip().split(' '))) for j in sentences] \n",
    "\n",
    "# co-occurence matrix (3292, 3292)\n",
    "C = np.zeros((len(artists), len(artists)))\n",
    "for sentence in sentenceslist: # j is a sentence (playlist)\n",
    "    for context in sentence: # a is context word (artsit)\n",
    "        for target in sentence: # b is target word (artsit)\n",
    "            C[context, target] += 1 # the number of times target word (artist) occurs with the context of word (artist) together\n",
    "\n",
    "np.fill_diagonal(C,0)  # for the same artist just set the co-occuurence to zero."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
