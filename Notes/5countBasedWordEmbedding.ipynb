{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count-based algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word-context matrix M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M \\in \\mathbb{R}^{|V_W|\\times |V_C|}$ is a word-context matrix \n",
    "\n",
    "each row $i$ corresponds to a word $\\mathbf{v}_w$\n",
    "\n",
    "each column $j$ corresponds to a context $\\mathbf{v}_c$\n",
    "\n",
    "entry of a word-context matrix $M$ is an correlation measure between the word $\\mathbf{v}_w$ and the context $\\mathbf{v}_c$\n",
    "\n",
    "$$\n",
    "M_{i,j} = f(\\mathbf{v}_w, \\mathbf{v}_c)= \\left \\langle \\mathbf{v}_w, \\mathbf{v}_c \\right \\rangle = \\mathbf{v}_w^T \\mathbf{v}_c \n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}_w \\in \\mathbb{R}^d$ is embedding of word $w \\in V_W$\n",
    "\n",
    "$\\mathbf{v}_c \\in \\mathbb{R}^d$ is embedding of context word $c \\in V_C$\n",
    "\n",
    "**the entries in the vectors are latent and are the params to be learned**\n",
    "\n",
    "$V_W$ is the word vocabulary\n",
    "\n",
    "$V_C$ is the context vocabulary\n",
    "\n",
    "for convenience, sometimes take $V=V_W=V_C$\n",
    "\n",
    "$|V_W|$ is the size of the word vocabulary $V_W$ \n",
    "\n",
    "$|V_C|$ is the size of the context vocabulary $V_C$\n",
    "\n",
    "size means the number of words in a vocabulary\n",
    "\n",
    "sometimes refer $\\mathbf{v}_w^T$ as rows in word embedding matrix $W \\in \\mathbb{R}^{|V_W|\\times d}$\n",
    "\n",
    "refer $\\mathbf{v}_c^T$ as rows in context embedding matrix $C \\in \\mathbb{R}^{|V_C|\\times d}$\n",
    "\n",
    "then $W_i$ refers to word embedding of ith word in the word vocabulary $V_W$\n",
    "\n",
    "$C_i$ refers to word embedding of ith context in the context vocabulary $V_C$\n",
    "\n",
    "rows of $W$ are often used in NLP tasks, such as compute word similarities while $C$ is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "- word and context can come from different vocabulary\n",
    "\n",
    "\n",
    "- a context may not occur in our corpus (unobserved/randomly sampled context)\n",
    "\n",
    "\n",
    "- our corpus is an observed collection of docs after segmantation and tokenization\n",
    "\n",
    "\n",
    "- $D$ is a collection of observed word-context pairs, which is **a subset of our corpus**,sometimes also called corpus\n",
    "\n",
    "- the words (observed words) in our corpus also belong to word vocabulary\n",
    "\n",
    "$$\n",
    "w_i \\in V_W\n",
    "$$\n",
    "\n",
    "- and corresponding contexts in our corpus also belong to context vocabulary\n",
    "    \n",
    "$$\n",
    "c_i \\in V_C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types of word-context matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PMI (pointwise mutual info matrix)\n",
    "\n",
    "\n",
    "- SPMI (**shifted** pointwise mutual info matrix)\n",
    "\n",
    "\n",
    "- SPPMI (shifted **positive** pointwise mutual info matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average mutual info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**average mutual info**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(w_1,w_2)\n",
    "\n",
    "&=\\sum _{w_1,w_2} p(w_1,w_2)\\log\\left [ \\frac{p(w_1,w_2)}{p(w_1)p(w_2)} \\right ] \\\\[1em]\n",
    "\n",
    "&=\\sum _{w_1} p(w_2|w_1)\\log\\left [ \\frac{p(w_2|w_1)}{p(w_2)} \\right ]\\\\[1em]\n",
    "\n",
    "&=H(w_2)-H(w_2|w_1)\\\\[1em]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $H$ is **entropy**\n",
    "\n",
    "$H(w_2)$ measures uncertainty of word $2$:\n",
    "\n",
    "$$\n",
    "H(w_2)=\\sum _{w_2} p(w_2)\\log\\left [ \\frac{1}{p(w_2)} \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI (pointwise mutual information)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduced by Church and Hanks\n",
    "\n",
    "PMI is an info-theoretic correlation measure between a pair of dicrete outcomes $x$ and $y$\n",
    "\n",
    "- definition:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log \\frac{P(x,y)}{P(x)P(y)}\n",
    "$$\n",
    "\n",
    "\n",
    "- in NLP context, $\\text{PMI}(w,c)$ measures the correlation between a word $w$ and a context $c$ \n",
    "\n",
    "    $PMI>0$: very likely occur together\n",
    "    \n",
    "    $PMI<0$: not very likely occur together\n",
    "\n",
    "    by calculating the log of the ratio between their **joint probability** (the frequency they occur together) and their **marginal probabilities**) (the frequency they occur independently)\n",
    "\n",
    "$$\n",
    "\\text{PMI}(w,c) = \\log \\frac{P(w,c)}{P(w)P(c)}= \\log \\left [\\frac{\\frac{\\#(w,c)}{|D|}}{\\frac{\\#(w)}{|D|}\\frac{\\#(c)}{|D|}}  \\right ]= \\log\\left [\\frac{\\#(w,c)\\cdot |D|}{\\#(w) \\cdot \\#(c)}  \\right ]\n",
    "$$\n",
    "\n",
    "where $\\#(w,c)$ is the number of times the specific word-context pair $(w,c)$ occurs in $D$\n",
    "\n",
    "$\\#(w)$ is the number of times the specific word $w$ occurs in $D$\n",
    "\n",
    "$$\n",
    "\\#(w) = \\sum_{c' \\in V_C} \\#(w,c')\n",
    "$$\n",
    "\n",
    "$\\#(c)$ is the number of times the specific context $c$ occurs in $D$\n",
    "\n",
    "$$\n",
    "\\#(c) = \\sum_{w' \\in V_W} \\#(w',c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dis of PMI matrix\n",
    "\n",
    "- entry of unobserved word-context pairs $(w,c)$ is $- \\infty$\n",
    "      \n",
    "$$\n",
    "PMI(w,c) = \\log\\left [\\frac{\\#(w,c)\\cdot |D|}{\\#(w) \\cdot \\#(c)}  \\right ] = \\log\\left [\\frac{0\\cdot |D|}{\\#(w) \\cdot \\#(c)}  \\right ] = \\log 0 = - \\infty\n",
    "$$\n",
    "    \n",
    "- dense: a major practical issue coz PMI's huge dimension $V_W \\times V_C$\n",
    "        \n",
    "\n",
    "- one smooth method:\n",
    "\n",
    "    use **Dirichlet prior** by adding a small \"fake\" count to PMI can make all word-context pairs **observed**\n",
    "    \n",
    "    but the PMI matrix is still dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the optimal solution of SGNS is **inplicitly factorizing a shifted PMI matrix**\n",
    "\n",
    "$$\n",
    "M = W C^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recall entry of a word-context matrix $M$ is an correlation measure between the word $\\mathbf{v}_w$ and the context $\\mathbf{v}_c$\n",
    "\n",
    "$$\n",
    "M_{i,j} = f(\\mathbf{v}_w, \\mathbf{v}_c)= \\left \\langle \\mathbf{v}_w, \\mathbf{v}_c \\right \\rangle = \\mathbf{v}_w^T \\mathbf{v}_c \n",
    "$$\n",
    "\n",
    "**but we don't know which correlation measure $f(\\mathbf{v}_w, \\mathbf{v}_c)$ SGNS is using**\n",
    "\n",
    "Thus $M$ is an **implicit** matrix, which comes the name **implicit matrix factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **local training objective** for a specific word-context pair $(w,c)$\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{v}}_w, \\hat {\\mathbf{v}}_c =\\arg \\max l(w,c) = \\arg \\max_{\\mathbf{v}_w, \\mathbf{v}_c}\\# (w,c)\\left[ \\log (\\sigma (\\mathbf{v}_w^T \\mathbf{v}_c )) + k \\cdot \\# (w) \\cdot \\frac{\\# (c)}{|D|} [\\log \\sigma (- \\mathbf{v}_w^T v_{c_N} )] \\right]\n",
    "$$\n",
    "\n",
    "derived from a sufficiently large dimensionality $d$\n",
    "\n",
    "which allows for a perfect reconstruction of $M$\n",
    "\n",
    "dot product of word-context pair can assume to be independent of each other\n",
    "\n",
    "thus we can treat the global objective $l$ as a function of independent $\\mathbf{v}_w^T \\mathbf{v}_c$ terms\n",
    "\n",
    "\n",
    "- define $x=\\mathbf{v}_w^T \\mathbf{v}_c$, set $\\frac{\\partial l}{\\partial x}=0$\n",
    "\n",
    "    the optimal local solution is \n",
    "\n",
    "$$\n",
    "x=\\mathbf{v}_w^T \\mathbf{v}_c = \\log \\left (\\frac{\\#(w,c) \\cdot |D| }{\\#(w) \\cdot \\#(c)} \\right )-\\log k\n",
    "$$\n",
    "\n",
    "interestingly, $\\log \\left (\\frac{\\#(w,c) \\cdot |D| }{\\#(w) \\cdot \\#(c)} \\right )$ is just the well-known pointwise mutual information (PMI) of $(w,c)$\n",
    "\n",
    "\n",
    "then we find the matrix $M$ that SGNS is factorizing:\n",
    "\n",
    "$$\n",
    "M_{ij}=W_i C_j = v_{w_i}^T v_{c_j} = \\text{PMI}(w_i,c_j ) -\\log k\n",
    "$$\n",
    "\n",
    "- $k$ is hyperparamter\n",
    "\n",
    "when $k=1$, $M$ is PMI matrix\n",
    "\n",
    "$$\n",
    "M^{\\text{PMI}} = \\text{PMI}(w,c)\n",
    "$$\n",
    "\n",
    "when $k>1$, $M$ is **shifted PMI matrix (SPMI)**\n",
    "\n",
    "$$\n",
    "M^{\\text{PMI}_k}  = M^{\\text{PMI}} -\\log k = \\text{PMI}(w,c) -\\log k \n",
    "$$\n",
    "\n",
    "**certain value of $k$ can improve performance on different tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it's impractical to directly use very high-dimensional and dense shifted PMI matrix\n",
    "\n",
    "- one solution: a **sparse but inconsistent** matrix $M_0^{PMI}$\n",
    "\n",
    "    replace all the entries of **unobserved** word-context pairs ($-\\infty$) with $0$\n",
    "\n",
    "$$\n",
    "PMI(w,c)=0 \\text{ if } \\#(w,c)=0\n",
    "$$\n",
    "\n",
    "- problem: **unobserved** word-context pairs have entries **0**\n",
    "\n",
    "    but **observed and uncorrelated** word-context pairs have **negative** entries\n",
    "\n",
    "\n",
    "- another better solution: a **sparse and consistent** matrix SPPMI\n",
    "\n",
    "    replace all the entries with **negative values** with $0$\n",
    "\n",
    "    thus the matrix is called **positive**\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w,c)=\\max(PMI(w,c), 0)\n",
    "$$\n",
    "\n",
    "\n",
    "- SPPMI is better at optimizing SGNS's training objective\n",
    "\n",
    "\n",
    "from PPMI, when $k>1$, we derive another embedding algorithm\n",
    "\n",
    "$$\n",
    "SPPMI_k(w,c)=\\max(PMI(w,c)-\\log k, 0)\n",
    "$$\n",
    "\n",
    "as SGND, certain value of $k$ can improve performance on different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intuition of discarding negative value: \n",
    "\n",
    "    humans can easily think of positive examples of positive association (e.g. Micky and Mouse) \n",
    "\n",
    "    but hard to invent negative examples of negative association (e.g., Micky and turkey). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCE (noise-contrastive estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to SGNS, NCE can also be viewed as implictly factorizing a word-contex matrix\n",
    "\n",
    "$M$ is a shifted log-conditional-probability matrix\n",
    "\n",
    "$$\n",
    "M  = \\log P(w|c) -\\log k\n",
    "$$\n",
    "\n",
    "where $\\log P(w|c)$ is a log-conditional-probability matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncated SVD is a basic algorithm from linear algebra to achieve optimal **rank d** factorization with respect to $l_2$ loss\n",
    "\n",
    "SVD factorize $M \\in \\mathbb{R}^{|V| \\times |V|}$ into product of 3 matrices\n",
    "\n",
    "$$\n",
    "M=U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U \\in \\mathbb{R}^{|V| \\times |V|}$ is an orthonormal matrix with columns are the left singular vectors of $M$\n",
    "\n",
    "$\\Sigma \\in \\mathbb{R}^{|V| \\times |V|}$ is a diagonal matrix with diagonal entries of singular values of $M$\n",
    "\n",
    "$V \\in \\mathbb{R}^{|V| \\times |V|}$ is an orthonormal matrix with columns are the right singular vectors of $M$\n",
    "\n",
    "recall 4th interpretation of PCA: low-rank (k) matrix approximation \n",
    "\n",
    "training goal: optimize over **rank d** matrix $M'$ to minimize $l_2$ reconstruction error\n",
    "\n",
    "$$\n",
    "\\hat M' = \\underset{\\text{Rank}(M')=d}{\\arg \\min}=\\left \\| M'-M \\right \\|_2^2\n",
    "$$\n",
    "\n",
    "the optimal solution is the rank d matrix $M_d \\in \\mathbb{R}^{|V| \\times |V|}$ that best approximate the original matrix $M$\n",
    "\n",
    "$$\n",
    "\\hat M' =M_d=U_d\\Sigma_d V_d^T\n",
    "$$\n",
    "\n",
    "where $U_d  \\in \\mathbb{R}^{|V| \\times d}$ is an orthonormal matrix with columns are the **top d** left singular vectors of $M$\n",
    "\n",
    "$\\Sigma_d \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix with **top d** diagonal entries of singular values of $M$\n",
    "\n",
    "$V_d \\in \\mathbb{R}^{|V| \\times d}$ is an orthonormal matrix with columns are the **top d** right singular vectors of $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus, given $|V| \\gg d$, the dense, d dimensional rows of $W=U_d \\Sigma_d$ is a perfect substitues for the very high-dimensional ($|V|$) rows of $M$\n",
    "\n",
    "$$\n",
    "W^{SVD}=U_d \\Sigma_d \\in \\mathbb{R}^{|V| \\times |V|}\\\\[1em]\n",
    "C^{SVD}=V_d \\in \\mathbb{R}^{|V| \\times |V|}\n",
    "$$\n",
    "\n",
    "\n",
    "- word embedding $\\mathbf{w}_i \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_i = [U_d \\Sigma_d]_i = W_i\n",
    "$$\n",
    "\n",
    "- context embedding $\\mathbf{c}_i \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_i = [V_d]_i = C_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### symmetric SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word embedding matrix $W^{SVD}$ and context embedding matrix $C^{SVD}$ have different properties:\n",
    "\n",
    "    $W^{SVD}$ is not orthonormal, $C^{SVD}$ is orthonormal\n",
    "\n",
    "\n",
    "- while factorization achieved by SGNS is more **symmetric**\n",
    "\n",
    "    neither $W^{W2V}$ nor $C^{W2V}$ is orthonormal\n",
    "\n",
    "    and no bias is given to either matrics in training objective\n",
    "\n",
    "\n",
    "- thus we propose another SVD algorithm: symmetric SVD\n",
    "\n",
    "$$\n",
    "W^{SVD_{1/2}}=U_d {\\Sigma_d}^{1/2}\\\\[1em]\n",
    "C^{SVD_{1/2}}=V_d {\\Sigma_d}^{1/2}\n",
    "$$\n",
    "\n",
    "- this algorithm perform better than previous SVD algorithm\n",
    "\n",
    "- this algorithm can be generalized to, making $\\alpha$ a tunable parameter\n",
    "\n",
    "$$\n",
    "W^{SVD_{\\alpha}}=U_d {\\Sigma_d}^{\\alpha}\\\\[1em]\n",
    "C^{SVD_{\\alpha}}=V_d {\\Sigma_d}^{\\alpha}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
